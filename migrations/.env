# ==================== migrations/env.py ====================
"""Alembic environment configuration"""

import asyncio
from logging.config import fileConfig
from sqlalchemy import pool
from sqlalchemy.engine import Connection
from sqlalchemy.ext.asyncio import async_engine_from_config
from alembic import context

# Import your models
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

from src.core.database import Base
from src.core.config import settings

# Import all models to ensure they're registered
from src.models import user, media, transaction

config = context.config
config.set_main_option("sqlalchemy.url", settings.database_url)

if config.config_file_name is not None:
    fileConfig(config.config_file_name)

target_metadata = Base.metadata


def run_migrations_offline() -> None:
    """Run migrations in 'offline' mode."""
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()


def do_run_migrations(connection: Connection) -> None:
    context.configure(connection=connection, target_metadata=target_metadata)

    with context.begin_transaction():
        context.run_migrations()


async def run_async_migrations() -> None:
    """Run migrations in 'online' mode."""
    connectable = async_engine_from_config(
        config.get_section(config.config_ini_section, {}),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    async with connectable.connect() as connection:
        await connection.run_sync(do_run_migrations)

    await connectable.dispose()


def run_migrations_online() -> None:
    """Run migrations in 'online' mode."""
    asyncio.run(run_async_migrations())


if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()



# ==================== scripts/deploy.sh ====================
#!/bin/bash
# Production deployment script

set -e

echo "🚀 Starting production deployment..."

# Load environment variables
if [ -f .env ]; then
    export $(cat .env | grep -v '^#' | xargs)
fi

# Check required variables
required_vars="BOT_TOKEN DB_PASSWORD REDIS_PASSWORD MINIO_ACCESS_KEY MINIO_SECRET_KEY"
for var in $required_vars; do
    if [ -z "${!var}" ]; then
        echo "❌ Error: $var is not set"
        exit 1
    fi
done

# Pull latest code
echo "📦 Pulling latest code..."
git pull origin main

# Build Docker images
echo "🔨 Building Docker images..."
docker-compose -f docker/docker-compose.yml build

# Run database migrations
echo "🗄️ Running database migrations..."
docker-compose -f docker/docker-compose.yml run --rm bot-api alembic upgrade head

# Start services
echo "🎯 Starting services..."
docker-compose -f docker/docker-compose.yml up -d

# Wait for services to be healthy
echo "⏳ Waiting for services to be healthy..."
sleep 10

# Check service health
services="postgres redis rabbitmq minio bot-api worker"
for service in $services; do
    if docker-compose -f docker/docker-compose.yml ps | grep -q "$service.*Up"; then
        echo "✅ $service is running"
    else
        echo "❌ $service failed to start"
        docker-compose -f docker/docker-compose.yml logs $service
        exit 1
    fi
done

echo "✅ Deployment complete!"


# ==================== scripts/backup.sh ====================
#!/bin/bash
# Backup script for database and media files

set -e

TIMESTAMP=$(date +%Y%m%d_%H%M%S)
BACKUP_DIR="/backups/$TIMESTAMP"

echo "🔐 Starting backup at $TIMESTAMP"

# Create backup directory
mkdir -p $BACKUP_DIR

# Backup PostgreSQL
echo "🗄️ Backing up database..."
docker-compose exec postgres pg_dump -U bot_user bot_db | gzip > "$BACKUP_DIR/database.sql.gz"

# Backup Redis
echo "💾 Backing up Redis..."
docker-compose exec redis redis-cli --rdb "$BACKUP_DIR/redis.rdb"

# Backup MinIO data
echo "📦 Backing up media files..."
docker-compose exec minio mc mirror minio/media-files "$BACKUP_DIR/media"

# Compress backup
echo "🗜️ Compressing backup..."
tar -czf "/backups/backup_$TIMESTAMP.tar.gz" -C /backups $TIMESTAMP

# Upload to remote storage (optional)
if [ ! -z "$BACKUP_S3_BUCKET" ]; then
    echo "☁️ Uploading to S3..."
    aws s3 cp "/backups/backup_$TIMESTAMP.tar.gz" "s3://$BACKUP_S3_BUCKET/backups/"
fi

# Clean old local backups (keep last 7 days)
echo "🧹 Cleaning old backups..."
find /backups -name "backup_*.tar.gz" -mtime +7 -delete

echo "✅ Backup completed successfully!"


# ==================== pytest.ini ====================
[pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
addopts = 
    -v
    --tb=short
    --strict-markers
    --cov=src
    --cov-report=term-missing
    --cov-report=html
    --cov-report=xml
    --asyncio-mode=auto
markers =
    unit: Unit tests
    integration: Integration tests
    e2e: End-to-end tests
    slow: Slow tests
    benchmark: Performance benchmark tests
    load: Load testing